## C16 — Coordination technique en Agile (Kanban) et MLOps (Projet E3‑E4)

Ce document formalise l’organisation Agile (Kanban) en mode solo et son articulation avec le contexte MLOps pour coordonner le développement et garantir les objectifs de production et de qualité.

---

### 1) Méthode appliquée et cadre

-   Méthode: Kanban (flux tiré, limitation du WIP, amélioration continue).
-   Contexte: développeur solo (porte les rôles opérationnels) + commanditaire (validation). Rôles assumés:
    -   Product Owner (PO): priorisation, définition des critères d’acceptation.
    -   Dev/Tech Lead: conception/développement, qualité, sécurité.
    -   MLOps Owner: monitoring IA/FAISS, coût/tokens, amélioration continue.
-   Cadence: micro‑itérations d’1 semaine avec synchronisations asynchrones (journal quotidien) et points hebdomadaires (planification/revue/rétro).

Processus de bout‑en‑bout (une carte Kanban):

```mermaid
flowchart LR
  DOR["Ready (DoR)"] --> DEV["En cours (WIP≤2)"] --> PR["Self‑PR/Review"] --> TEST["Tests & Qualité"] --> DEP["Intégration pré‑prod"] --> MON["Monitoring & Feedback"] --> DONE["Terminé"]
```

Définitions:

-   DoR (Definition of Ready): description claire, critères d’acceptation mesurables, dépendances identifiées, impact accessibilité.
-   DoD (Definition of Done): tests verts, lint OK, sécurité/OWASP vérifiée, docs à jour, métriques exposées si endpoints.

---

### 2) Cycles, étapes, rituels et objectifs

-   Cycle hebdomadaire (cadencé, mais Kanban reste flux continu):
    -   Lundi (Planning 30 min): sélectionner les cartes « Ready » → « En cours » (WIP≤2), ajuster priorités.
    -   Quotidien (10 min async): journal de bord (hier/aujourd’hui/risques) dans `E3-E4/LOGS.md`.
    -   Jeudi (Revue 20 min): démonstration locale (POC pré‑prod) et validation des critères d’acceptation.
    -   Vendredi (Rétrospective 15 min): ce qui a bien/mal fonctionné, actions d’amélioration (WIP, qualité, accessibilité).
-   Rituels MLOps (hebdo, 15 min): lecture des métriques (latence p95, taux d’erreurs, tokens), décisions (prompts, modèle, limites).
-   Objectifs rappelés à chaque rituel: qualité (tests/OWASP), accessibilité (WCAG), coût (tokens), performance (p95), maintenabilité.

---

### 3) Outils de pilotage et accessibilité des éléments

-   Tableau Kanban (GitHub Projects ou Trello): colonnes « Backlog → Ready → En cours (WIP 2) → Review → Validation (CI) → Terminé ».
-   Backlog: GitHub Issues avec labels `type:{story,tech,bug,spike}`, `area:{auth,chat,faiss,ui,monitoring,ci}`, `accessibility`.
-   Indicateurs visuels:
    -   Cumulative Flow Diagram (CFD) via Projects (ou export) pour surveiller en‑cours/flux.
    -   Milestones (option): mini burndown par lot pour date cible.
-   Artefacts accessibles (dans le dépôt):
    -   `E3-E4/C14.md` (besoin et modélisations), `E3-E4/C15.md` (cadre technique), `E3-E4/C17.md` (développement), `E3-E4/MONITORING.md` (observabilité), `E3-E4/CI_CD_GUIDE.md` (CI/CD).
    -   `README.md` (lancement), `fastapi/tests/*` (couverture), dashboards Grafana provisionnés (`monitoring/grafana`).

---

### 4) Qualité, sécurité et critères d’acceptation

-   Gates qualité (avant passage « Terminé »):
    -   Tests: `pytest -v` verts; couverture ciblée (endpoints métier + accès).
    -   Lint: style/format; dépendances revues; secrets hors dépôt.
    -   Sécurité: CORS, cookies HttpOnly/SameSite, validation Pydantic, hachage bcrypt, revue OWASP Top 10 pertinente.
    -   Accessibilité: contrôle manuel (clavier, focus, contrastes, labels); objectifs WCAG notés dans l’issue.
-   Check‑list self‑PR (résumé): portée, risques, validation, captures (UI/dashboards), items sécurité/accessibilité, impacts monitoring.

---

### 5) Intégration MLOps (opérationnalisation IA)

-   Observabilité IA: métriques OpenAI/FAISS exposées (`/metrics`), collectées par Prometheus, visibles Grafana; alertes basiques.
-   Gouvernance prompts/modèles: variables d’env (`OPENAI_MODEL`), journal des changements dans `E3-E4/LOGS.md` (date, motif, effet sur p95/tokens).
-   Coûts: suivi via tokens (requêtes/réponses) + estimation locale; validation croisée avec tableau de bord OpenAI.
-   Boucle d’amélioration: incidents/latence → ajustements prompts/timeout/k FAISS → suivi métriques → décision itérative.

---

### 6) Plan de mise en œuvre (solo, crédible)

1. Initialiser le board (colonnes, WIP=2) et créer le backlog minimal par lots (POC, Qualité/Observabilité, Industrialisation).
2. Établir DoR/DoD dans la description du projet; créer étiquettes standard et modèles d’issues.
3. Démarrer le flux: tirer 1‑2 cartes max; journal quotidien dans `LOGS.md`.
4. Hebdo: planning (sélection), revue (démo pré‑prod Docker), rétro (actions ciblées).
5. MLOps hebdo: revue métriques et décisions (prompts/modèles/limites tokens).

---

### 7) Conformité aux attendus C16

-   Cycles/étapes/rôles/rituels/outils définis et respectés (Kanban + rituels hebdo + MLOps).
-   Outils de pilotage disponibles: board Kanban, backlog issues, CFD/burndown (milestones), artefacts versionnés.
-   Objectifs et modalités des rituels documentés ci‑dessus et rappelés dans le board/`LOGS.md`.
-   Accessibilité des éléments: tout est versionné dans le dépôt (`C14.md`, `C15.md`, `C16.md`, `C17.md`, monitoring, CI/CD), consultable en continu.
