## Monitoring de lâ€™application dâ€™IA â€” Rapport de certification (C20)

Ce document prÃ©sente lâ€™architecture et la mise en Å“uvre du monitorage de lâ€™application dâ€™intelligence artificielle dÃ©veloppÃ©e dans le cadre du projet `E3-E4`. Lâ€™application est constituÃ©e dâ€™une API FastAPI intÃ©grant un modÃ¨le LLM (OpenAI), un moteur de recherche sÃ©mantique basÃ© sur FAISS et une base de donnÃ©es PostgreSQL. ConformÃ©ment Ã  la compÃ©tence C20, lâ€™objectif du dispositif de monitoring est de surveiller la disponibilitÃ© et les performances des composants, de dÃ©tecter automatiquement les incidents et dâ€™alimenter une boucle de rÃ©troaction MLOps permettant lâ€™amÃ©lioration continue du systÃ¨me, tout en respectant les principes de protection des donnÃ©es personnelles.

### Architecture de monitorage

Lâ€™architecture de surveillance repose sur la collecte de mÃ©triques instrumentÃ©es au sein de lâ€™API et lâ€™agrÃ©gation de mÃ©triques systÃ¨mes et bases de donnÃ©es, lesquelles sont centralisÃ©es puis visualisÃ©es. ConcrÃ¨tement, lâ€™application FastAPI expose un endpoint `/metrics` conforme au format Prometheus. Un service Prometheus, dÃ©ployÃ© en environnement local via Docker Compose, interroge pÃ©riodiquement cet endpoint ainsi que deux exporters standards: un exporter PostgreSQL pour les mÃ©triques de la base et un node exporter pour lâ€™hÃ´te. Les sÃ©ries temporelles collectÃ©es sont consultÃ©es depuis un service Grafana, qui propose des tableaux de bord prÃ©â€‘provisionnÃ©s et un moteur dâ€™alerting. Cette architecture, simple et modulaire, garantit une sÃ©paration claire des responsabilitÃ©s: lâ€™application se concentre sur lâ€™exposition de signaux pertinents, tandis que Prometheus et Grafana assurent respectivement la collecte, la conservation et lâ€™exploitation des donnÃ©es de supervision.

La configuration de Prometheus prÃ©cise des jobs de scraping pour lâ€™application, lâ€™exporter PostgreSQL et le node exporter, avec un intervalle adaptÃ© Ã  la criticitÃ© de chaque source. Lâ€™extrait suivant illustre le paramÃ©trage du job dÃ©diÃ© Ã  lâ€™API FastAPI, qui interroge lâ€™endpoint `/metrics` toutes les dix secondes.

```yaml
scrape_configs:
    - job_name: "fastapi-app"
      static_configs:
          - targets: ["web:8000"]
      metrics_path: "/metrics"
      scrape_interval: 10s
```

### Instrumentation applicative (FastAPI, OpenAI, FAISS)

Lâ€™instrumentation applicative est rÃ©alisÃ©e au plus prÃ¨s des parcours utilisateurs et des intÃ©grations IA. Un middleware HTTP dans `app.py` mesure le volume de requÃªtes, la latence par point dâ€™entrÃ©e et le nombre de requÃªtes en cours, et catÃ©gorise les erreurs en distinguant erreurs clientes et erreurs serveur. Cette approche permet dâ€™obtenir une vision fiable de lâ€™expÃ©rience perÃ§ue par les utilisateurs, notamment via le suivi de percentiles de latence. Auâ€‘delÃ  de la couche HTTP, lâ€™application instrumente les appels au modÃ¨le OpenAI ainsi que les recherches FAISS. Les mÃ©triques exposÃ©es incluent le nombre de requÃªtes LLM par modÃ¨le et par endpoint, la durÃ©e dâ€™exÃ©cution des appels, le volume de tokens envoyÃ©s et reÃ§us, la durÃ©e des recherches FAISS et le nombre de rÃ©sultats retournÃ©s. Lâ€™API expose Ã©galement des compteurs dÃ©crivant lâ€™activitÃ© conversationnelle (conversations par statut et messages par type) afin de relier lâ€™activitÃ© fonctionnelle aux indicateurs techniques. Lâ€™endpoint dâ€™exposition des mÃ©triques est minimaliste et sâ€™appuie directement sur la bibliothÃ¨que `prometheus_client`.

```python
# E3-E4/fastapi/app.py
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

### MÃ©triques Ã  risque et seuils dâ€™alerte

Le dispositif dâ€™alerte sâ€™appuie sur les rÃ¨gles de Grafana, provisionnÃ©es de maniÃ¨re dÃ©clarative. Trois scÃ©narios dâ€™incident majeurs sont couverts. Dâ€™abord, lâ€™Ã©lÃ©vation anormale du taux dâ€™erreurs HTTP est dÃ©tectÃ©e par une rÃ¨gle calculant le taux dâ€™occurrence des rÃ©ponses dont le code est de la famille 4xx ou 5xx sur une fenÃªtre de cinq minutes; un dÃ©passement du seuil fixÃ© Ã  dix pour cent maintenu pendant deux minutes dÃ©clenche une alerte de sÃ©vÃ©ritÃ© Â« warning Â». Ensuite, la dÃ©gradation des performances est surveillÃ©e au travers du quantile 95 de la distribution des latences HTTP; lorsque le p95 excÃ¨de deux secondes pendant deux minutes sur une fenÃªtre glissante de cinq minutes, une alerte Â« warning Â» est Ã©mise. Enfin, lâ€™indisponibilitÃ© du service est traitÃ©e par une rÃ¨gle sâ€™appuyant sur la mÃ©trique `up` issue de Prometheus: si le job `fastapi-app` est indisponible pendant une minute, une alerte de sÃ©vÃ©ritÃ© Â« critical Â» est gÃ©nÃ©rÃ©e. Ces seuils sont volontairement pragmatiques pour lâ€™environnement local et peuvent Ãªtre durcis en production en fonction des objectifs de service.

Ã€ titre dâ€™illustration, lâ€™expression PromQL utilisÃ©e pour dÃ©tecter un temps de rÃ©ponse excessif sâ€™appuie sur la fonction `histogram_quantile` appliquÃ©e au taux des buckets dâ€™histogramme de latence: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2`. De maniÃ¨re similaire, le taux dâ€™erreurs est Ã©valuÃ© via `rate(http_requests_total{status=~"4..|5.."}[5m]) > 0.1`, tandis que la dÃ©tection dâ€™indisponibilitÃ© repose sur `up{job="fastapi-app"} == 0`. Les notifications sont Ã©mises vers un canal Discord par lâ€™intermÃ©diaire dâ€™un Â« contact point Â» configurÃ© Ã  partir de la variable dâ€™environnement `DISCORD_WEBHOOK_URL`.

```yaml
# E3-E4/monitoring/grafana/provisioning/alerting/contact-points.yaml
contactPoints:
    - name: discord-default
      receivers:
          - uid: discord-webhook
            type: discord
            settings:
                url: "${DISCORD_WEBHOOK_URL}"
                message: |
                    ğŸš¨ Alerte Grafana: {{ .CommonLabels.alertname }}\nStatut: {{ .Status }}
```

### DÃ©ploiement et exploitation locale

Le dispositif est entiÃ¨rement opÃ©rable en environnement local au moyen de Docker Compose. Une fois les variables dâ€™environnement renseignÃ©es (voir `.env_example`, notamment `DISCORD_WEBHOOK_URL`, `OPENAI_API_KEY` et `OPENAI_MODEL`), le lancement de la commande `docker-compose up -d` provisionne lâ€™application, Prometheus, Grafana ainsi que les exporters PostgreSQL et Node. Prometheus conserve les sÃ©ries temporelles avec une rÃ©tention adaptÃ©e Ã  lâ€™environnement de dÃ©veloppement (environ deux cents heures) et expose sa console sur `http://localhost:9090`. Grafana est accessible sur `http://localhost:3000` avec des identifiants par dÃ©faut en local et charge automatiquement la source de donnÃ©es Prometheus ainsi que les tableaux de bord fournis, dont un tableau de bord dÃ©diÃ© Ã  FastAPI. Lâ€™application est accessible Ã  lâ€™adresse `http://localhost:8001`, tandis que le scraping des mÃ©triques sâ€™effectue au sein du rÃ©seau Docker sur `web:8000/metrics`.

### Journalisation et conformitÃ© des donnÃ©es personnelles

La journalisation applicative utilise le module standard `logging` et se limite Ã  des Ã©vÃ©nements techniques contextualisÃ©s (succÃ¨s ou Ã©chec dâ€™appel OpenAI, durÃ©es dâ€™exÃ©cution FAISS, statut des conversations), sans journaliser de contenu conversationnel ou de donnÃ©es identifiantes. Les mÃ©triques exposÃ©es par `prometheus_client` sont agrÃ©gÃ©es et utilisent des labels non identifiants (mÃ©thode, endpoint, statut HTTP, modÃ¨le), ce qui satisfait au principe de minimisation des donnÃ©es. Les secrets et informations sensibles sont fournis par variables dâ€™environnement, et lâ€™accÃ¨s Ã  lâ€™endpoint `/metrics` est conÃ§u pour rester interne au rÃ©seau dans une perspective de dÃ©ploiement en production. Ces choix assurent un niveau de conformitÃ© cohÃ©rent avec les exigences de protection des donnÃ©es dans un contexte dâ€™IA appliquÃ©e.

### Suivi et consultation des logs

En dÃ©veloppement, les logs de lâ€™application et des services sont consultÃ©s directement dans Docker Desktop (onglet Â« Logs Â» de chaque conteneur) ou en ligne de commande, selon prÃ©fÃ©rence.

```bash
docker compose logs -f web
docker compose logs -f prometheus grafana
```

En production, les logs sont consultÃ©s via Portainer dans la vue Â« Logs Â» de chaque conteneur. La consultation en CLI reste possible si nÃ©cessaire.

```bash
docker logs -f <container_name>
```

Ce choix privilÃ©gie la simplicitÃ©: je connais des solutions de centralisation comme Grafana Loki avec Promtail (ou des stacks ELK/EFK), trÃ¨s utiles pour la recherche pleinâ€‘texte, la corrÃ©lation et la rÃ©tention avancÃ©e, mais elles seraient surdimensionnÃ©es pour cette application. Elles ajoutent des composants Ã  exploiter (agents, stockage, politiques de rÃ©tention) sans bÃ©nÃ©fice immÃ©diat. Les mÃ©triques et alertes couvrent les signaux forts; lâ€™inspection des logs conteneurisÃ©s via Docker Desktop et Portainer suffit aujourdâ€™hui. Si la volumÃ©trie, les exigences dâ€™audit ou les SLO Ã©voluent, une centralisation pourra Ãªtre introduite ultÃ©rieurement sans modification du code applicatif.

### ProcÃ©dures de test des alertes

La validitÃ© des rÃ¨gles dâ€™alerte est vÃ©rifiÃ©e par des scÃ©narios dâ€™inductions contrÃ´lÃ©es. Lâ€™indisponibilitÃ© du service est testÃ©e en arrÃªtant temporairement le conteneur `web`, ce qui entraÃ®ne, aprÃ¨s une minute, lâ€™Ã©mission dâ€™une alerte critique Â« ServiceDown Â». Le taux dâ€™erreurs est Ã©prouvÃ© en forÃ§ant des rÃ©ponses 5xx sur un endpoint de test pendant plus de deux minutes, permettant dâ€™observer le dÃ©clenchement de lâ€™alerte Â« HighErrorRate Â». La surveillance de la latence est Ã©valuÃ©e en introduisant, cÃ´tÃ© application, une temporisation volontaire portant la durÃ©e de traitement auâ€‘delÃ  de deux secondes, puis en soumettant la route concernÃ©e Ã  une charge continue pendant environ cinq minutes; lâ€™alerte Â« HighResponseTime Â» doit alors apparaÃ®tre. Dans chaque cas, la rÃ©ception des notifications sur Discord et lâ€™Ã©volution de lâ€™Ã©tat dans Grafana (ouverture, acquittement, rÃ©solution) sont contrÃ´lÃ©es.

### Boucle de rÃ©troaction MLOps

Les mÃ©triques et alertes alimentent une boucle de rÃ©troaction qui structure lâ€™amÃ©lioration continue du systÃ¨me. Les indicateurs de latence et dâ€™erreur des appels OpenAI conduisent Ã  revisiter les choix de modÃ¨les, les paramÃ¨tres de timeout, la stratÃ©gie de retry et la conception des prompts. Les mesures de performance de FAISS, notamment les temps de recherche et le nombre de rÃ©sultats pertinents retournÃ©s, Ã©clairent lâ€™optimisation de lâ€™index, la valeur de `k` et la qualitÃ© du corpus indexÃ©. Le suivi des volumes de tokens permet de maÃ®triser les coÃ»ts en adaptant la longueur du contexte et en rationalisant les prompts. Enfin, lâ€™analyse du taux dâ€™erreurs HTTP et des exceptions corrÃ©lÃ©es aboutit Ã  des correctifs ciblÃ©s sur les endpoints et Ã  lâ€™introduction de mÃ©canismes de repli et de backoff. Cette dÃ©marche est consignÃ©e et opÃ©rÃ©e de maniÃ¨re itÃ©rative pour renforcer conjointement robustesse, performance et efficience Ã©conomique.

### Alignement avec la compÃ©tence C20

Le dispositif prÃ©sentÃ© rÃ©pond aux attendus de la compÃ©tence C20. Les mÃ©triques clÃ©s liÃ©es aux risques (erreurs, latence, disponibilitÃ©, coÃ»ts via tokens) sont dÃ©finies et assorties de seuils et dâ€™alertes effectifs. Les choix technologiques â€” instrumentation `prometheus_client` dans lâ€™API, collecte par Prometheus, visualisation et alerting par Grafana, exporters Postgres et Node â€” sont justifiÃ©s par leur maturitÃ©, leur faible empreinte et leur interopÃ©rabilitÃ©. Lâ€™ensemble des outils est installÃ© et opÃ©rationnel en environnement local, et lâ€™instrumentation nÃ©cessaire est intÃ©grÃ©e au code source dans `app.py` et `monitoring_utils.py`. Les alertes, enfin, sont configurÃ©es et fonctionnelles avec un acheminement vers Discord; elles constituent le point de dÃ©part dâ€™actions correctrices concrÃ¨tes dans la boucle MLOps. Le respect de la protection des donnÃ©es personnelles est pris en compte par la minimisation des informations exposÃ©es, lâ€™absence de journalisation de contenu sensible et lâ€™usage de variables dâ€™environnement pour les secrets.

### Annexes techniques

Lâ€™exemple suivant rappelle le principe dâ€™exposition des mÃ©triques cÃ´tÃ© application, qui repose sur la gÃ©nÃ©ration du format Prometheus Ã  partir du registre interne de `prometheus_client`.

```python
# E3-E4/fastapi/app.py
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

Le contact de notification Grafana est dÃ©fini pour diffuser les alertes vers Discord Ã  lâ€™aide dâ€™un webhook configurable par variable dâ€™environnement.

```yaml
# E3-E4/monitoring/grafana/provisioning/alerting/contact-points.yaml
contactPoints:
    - name: discord-default
      receivers:
          - uid: discord-webhook
            type: discord
            settings:
                url: "${DISCORD_WEBHOOK_URL}"
                message: |
                    ğŸš¨ Alerte Grafana: {{ .CommonLabels.alertname }}\nStatut: {{ .Status }}
```

Pour la collecte, Prometheus interroge lâ€™API Ã  un intervalle de dix secondes, ce qui fournit une rÃ©solution suffisante pour lâ€™observation fine des pics de latence et des incidents en environnement de dÃ©veloppement.

```yaml
# E3-E4/monitoring/prometheus/prometheus.yml
scrape_configs:
    - job_name: "fastapi-app"
      static_configs:
          - targets: ["web:8000"]
      metrics_path: "/metrics"
      scrape_interval: 10s
```

### Fonctionnement de Prometheus

Prometheus adopte un modÃ¨le de collecte de type Â« pull Â» dans lequel le serveur Prometheus interroge pÃ©riodiquement des cibles HTTP exposant des mÃ©triques au format texte. Chaque cible â€” lâ€™application FastAPI, lâ€™exporter PostgreSQL et le node exporter â€” publie un endpoint HTTP (`/metrics`) dÃ©crivant lâ€™Ã©tat de la cible sous forme de sÃ©ries temporelles nommÃ©es, enrichies de labels. Ã€ chaque scrutation, Prometheus Ã©chantillonne la valeur courante de ces sÃ©ries et les stocke dans sa base de donnÃ©es temporelle (TSDB) locale, en conservant pour chaque point un horodatage, une valeur et le jeu de labels associÃ©. La TSDB organise les donnÃ©es par Â« chunks Â» sur disque avec un mÃ©canisme de compaction, ce qui permet des requÃªtes temporelles efficaces et une rÃ©tention configurable adaptÃ©e aux contraintes de lâ€™environnement.

Les types de mÃ©triques standard â€” Counter, Gauge, Histogram et Summary â€” rÃ©pondent Ã  des usages distincts. Les compteurs sont strictement croissants et mesurent des Ã©vÃ©nements (par exemple `http_requests_total`). Les jauges reprÃ©sentent des grandeurs instantanÃ©es pouvant monter ou descendre (par exemple `http_requests_in_flight`). Les histogrammes accumulent des observations dans des Â« buckets Â» de seuils croissants et exposent automatiquement des sÃ©ries `*_bucket`, `*_count` et `*_sum` permettant dâ€™estimer des quantiles avec `histogram_quantile`. Les summaries calculent localement des quantiles, mais dans ce projet, les histogrammes ont Ã©tÃ© privilÃ©giÃ©s pour permettre lâ€™agrÃ©gation cÃ´tÃ© serveur. Dans notre configuration, lâ€™intervalle de scrutation est de dix secondes pour lâ€™application, ce qui offre une rÃ©solution suffisante pour suivre les pics de latence et de charge en phase de dÃ©veloppement, tout en limitant la cardinalitÃ© et le volume de donnÃ©es.

Lors de lâ€™exÃ©cution, Prometheus Ã©tiquette chaque sÃ©rie avec des labels clÃ©/valeur (par exemple `method`, `endpoint`, `status` pour les mÃ©triques HTTP, `model` et `endpoint` pour les mÃ©triques OpenAI). Ces labels rendent possible des agrÃ©gations par dimension (somme ou moyenne par endpoint, quantiles par modÃ¨le, etc.). Le serveur expose ensuite une API de requÃªtage (PromQL) utilisÃ©e Ã  la fois par lâ€™interface Prometheus et par Grafana pour rÃ©aliser des graphiques, des statistiques et des rÃ¨gles dâ€™alerte. Enfin, la mÃ©trique spÃ©ciale `up` permet de vÃ©rifier la disponibilitÃ© dâ€™une cible: une valeur de `1` indique que le scrape a rÃ©ussi, `0` signale une indisponibilitÃ©.

### Fonctionnement de Grafana

Grafana se connecte Ã  Prometheus en tant que source de donnÃ©es et fournit une couche de visualisation, de tableaux de bord et dâ€™alerting. Dans ce projet, la source de donnÃ©es Prometheus est provisionnÃ©e automatiquement au dÃ©marrage de Grafana. Les tableaux de bord sont dÃ©crits au format JSON et montÃ©s dans le conteneur, puis Grafana les charge selon la configuration de provisioning. Chaque panneau dâ€™un tableau de bord contient une ou plusieurs requÃªtes PromQL et un rendu (courbe temporelle, statistique unique, jauge, etc.). Grafana permet dâ€™appliquer des transformations simples (agrÃ©gations, changements dâ€™unitÃ©, renommage), de configurer des seuils visuels et de dÃ©finir des intervalles de rafraÃ®chissement.

Le moteur dâ€™alerting de Grafana Ã©value pÃ©riodiquement des expressions PromQL associÃ©es Ã  des conditions sur une durÃ©e. Lorsquâ€™une condition est satisfaite pendant la fenÃªtre dâ€™Ã©valuation, une alerte passe Ã  lâ€™Ã©tat Â« firing Â». Les notifications sont expÃ©diÃ©es via des Â« contact points Â»; ici, un webhook Discord paramÃ©trÃ© par la variable `DISCORD_WEBHOOK_URL` diffuse un message rÃ©sumant lâ€™alerte, sa gravitÃ© et un lien de contexte vers Grafana. Cette approche centralise la gestion des alertes au mÃªme endroit que les dashboards tout en se reposant sur Prometheus pour lâ€™exÃ©cution des requÃªtes sousâ€‘jacentes.

### IntÃ©gration et exploitation des mÃ©triques OpenAI

Les mÃ©triques OpenAI sont instrumentÃ©es directement dans lâ€™API FastAPI. Des compteurs et histogrammes sont dÃ©clarÃ©s dans `app.py` avec les labels `model` et `endpoint`. Des dÃ©corateurs et context managers dÃ©finis dans `monitoring_utils.py` enveloppent les appels au LLM et enregistrent pour chaque exÃ©cution le succÃ¨s ou lâ€™Ã©chec (`openai_requests_total` avec `status`), la durÃ©e dâ€™exÃ©cution (`openai_request_duration_seconds` via la sÃ©rie `*_bucket`) ainsi que le volume de tokens utilisÃ©s (`openai_request_tokens` et `openai_response_tokens` lorsque lâ€™information est disponible). Lâ€™application expose ces sÃ©ries sur `/metrics` et Prometheus les scrute Ã  intervalle rÃ©gulier. Dans Grafana, les panneaux dÃ©diÃ©s se contentent dâ€™interroger Prometheus en PromQL.

Le taux de requÃªtes OpenAI sâ€™obtient par la dÃ©rivÃ©e temporelle du compteur `openai_requests_total` avec la fonction `rate` sur une fenÃªtre de cinq minutes. La latence perÃ§ue est estimÃ©e par `histogram_quantile(0.95, rate(openai_request_duration_seconds_bucket[5m]))`, qui reconstruit le quantile 95 Ã  partir des buckets dâ€™histogramme. Il est ensuite possible de filtrer par modÃ¨le ou endpoint Ã  lâ€™aide des labels, ou de regrouper les sÃ©ries pour calculer des agrÃ©gations par dimension. Si lâ€™on souhaite suivre les coÃ»ts, il suffit dâ€™ajouter un panneau interrogeant `increase(openai_request_tokens[5m])` et `increase(openai_response_tokens[5m])` pour mesurer la consommation de tokens sur la fenÃªtre.

### Explication des mÃ©triques des dashboards Grafana

Le tableau de bord Â« FastAPI Monitoring Dashboard Â» regroupe les principaux indicateurs de lâ€™API et des intÃ©grations IA. Le panneau Â« Request Rate Â» reprÃ©sente le dÃ©bit de requÃªtes HTTP, calculÃ© par `rate(http_requests_total[5m])` et ventilÃ© par mÃ©thode et endpoint. Il permet dâ€™identifier les pÃ©riodes de charge et de vÃ©rifier la stabilitÃ© du trafic. Le panneau Â« Response Time Â» affiche le 95e percentile des temps de rÃ©ponse Ã  lâ€™aide de `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))`. Une Ã©lÃ©vation persistante de ce quantile traduit une dÃ©gradation de lâ€™expÃ©rience utilisateur, et constitue souvent un prÃ©curseur dâ€™incident plus large. Le panneau Â« Error Rate Â» suit le taux dâ€™erreurs HTTP (codes 4xx et 5xx) via `rate(http_requests_total{status=~"4..|5.."}[5m])`. Une montÃ©e des 5xx signale un problÃ¨me serveur; les 4xx peuvent rÃ©vÃ©ler un dÃ©faut de validation ou dâ€™usage cÃ´tÃ© client.

Un panneau de type statistique, Â« Active Connections Â», agrÃ¨ge `http_requests_in_flight` pour fournir une mesure instantanÃ©e de la charge en cours de traitement. Deux panneaux dâ€™infrastructure, Â« CPU Usage Â» et Â« Memory Usage Â», proviennent du node exporter. Le premier estime lâ€™utilisation CPU par `100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`, ce qui reflÃ¨te la part non idle du CPU. Le second calcule le pourcentage de mÃ©moire utilisÃ©e Ã  partir des mÃ©triques `node_memory_MemTotal_bytes` et `node_memory_MemAvailable_bytes`. Ces deux panneaux permettent de corrÃ©ler les anomalies applicatives avec la saturation Ã©ventuelle de la machine hÃ´te.

Les intÃ©grations IA disposent de trois panneaux dÃ©diÃ©s. Â« OpenAI Requests Rate Â» retrace le dÃ©bit dâ€™appels LLM via `rate(openai_requests_total[5m])`, libellÃ© par modÃ¨le, endpoint et statut; il met en Ã©vidence la charge adressÃ©e au fournisseur et la proportion dâ€™erreurs. Â« OpenAI Response Time Â» prÃ©sente le p95 des durÃ©es des appels au modÃ¨le grÃ¢ce Ã  `histogram_quantile(0.95, rate(openai_request_duration_seconds_bucket[5m]))`, indicateur clef pour lâ€™UX et les timeouts. Â« FAISS Search Duration Â» restitue le p95 des durÃ©es de recherche via `histogram_quantile(0.95, rate(faiss_search_duration_seconds_bucket[5m]))`. Une hausse simultanÃ©e de ces latences oriente lâ€™investigation vers la saturation I/O, la taille de contexte, ou la configuration de lâ€™index FAISS. Deux panneaux de synthÃ¨se, Â« Chatbot Conversations Â» et Â« Chatbot Messages Â», exposent respectivement `chatbot_conversations_total` (par statut) et `chatbot_messages_total` (par type). Ils relient lâ€™activitÃ© fonctionnelle Ã  la santÃ© technique du systÃ¨me.

Le tableau de bord Â« PostgreSQL Monitoring Dashboard Â» se focalise sur lâ€™activitÃ© de la base. Le panneau Â« Active Connections Â» affiche `pg_stat_database_numbackends`, indicateur de connexions actives par base. Une croissance rapide peut signaler un pool mal dimensionnÃ© ou des fuites de connexions. Â« Database Size Â» sâ€™appuie sur `pg_database_size_bytes` pour suivre lâ€™Ã©volution du volume occupÃ© par les donnÃ©es, utile pour planifier la capacitÃ© et dÃ©tecter une croissance inattendue. Â« Transactions per Second Â» combine `rate(pg_stat_database_xact_commit[5m])` et `rate(pg_stat_database_xact_rollback[5m])` pour fournir un dÃ©bit global de transactions, reflet direct de lâ€™activitÃ© applicative. Enfin, Â« Cache Hit Ratio Â» mesure lâ€™efficacitÃ© du cache en calculant `rate(pg_stat_database_blks_hit[5m]) / (rate(pg_stat_database_blks_hit[5m]) + rate(pg_stat_database_blks_read[5m]))`. Un ratio Ã©levÃ©, proche de 1, indique que la plupart des lectures sont servies depuis le cache; une chute durable invite Ã  revoir la taille des buffers ou les schÃ©mas dâ€™accÃ¨s.

Dans lâ€™ensemble, ces dashboards offrent une vue cohÃ©rente de boutâ€‘enâ€‘bout: charge et qualitÃ© de service cÃ´tÃ© HTTP, performance et coÃ»t cÃ´tÃ© LLM, pertinence et latence cÃ´tÃ© FAISS, ainsi que lâ€™Ã©tat de lâ€™infrastructure et de la base de donnÃ©es. Leur lecture conjointe permet de diagnostiquer efficacement les incidents, dâ€™identifier les goulets dâ€™Ã©tranglement et dâ€™alimenter la boucle dâ€™amÃ©lioration continue MLOps en donnÃ©es objectives.
