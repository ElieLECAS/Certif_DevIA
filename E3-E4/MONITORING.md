## C11 â€” Monitorer un modÃ¨le dâ€™intelligence artificielle

Cette section rÃ©pond aux attendus de la compÃ©tence C11 et se concentre sur le monitorage Â« IA Â» au sein du projet `E3-E4`, avec un focus sur:

-   les appels au modÃ¨le OpenAI (dÃ©bit, latence, erreurs, tokens/coÃ»ts);
-   les recherches FAISS (dÃ©bit, latence, taille/rÃ©sultats);
-   la collecte, lâ€™alerte et la restitution via Prometheus/Grafana en temps rÃ©el;
-   lâ€™appui sur les mÃ©triques officielles du dashboard OpenAI comme source de vÃ©ritÃ© de coÃ»t/consommation.

### 1) MÃ©triques monitorÃ©es et interprÃ©tation

Les mÃ©triques ciâ€‘dessous sont exposÃ©es par lâ€™API au format Prometheus (endpoint `/metrics`) et consommÃ©es par Grafana. Elles sont choisies pour Ã©viter toute ambiguÃ¯tÃ© dâ€™interprÃ©tation.

-   OpenAI â€” volume de requÃªtes: `openai_requests_total{model,endpoint,status}`
    -   InterprÃ©tation: dÃ©rivÃ©e temporelle via `rate()` pour le dÃ©bit dâ€™appels; le label `status` distingue succÃ¨s/Ã©chec.
-   OpenAI â€” latence p95: histogramme `openai_request_duration_seconds_bucket{model,endpoint}`
    -   InterprÃ©tation: `histogram_quantile(0.95, rate(..._bucket[5m]))` pour le 95e percentile.
-   OpenAI â€” tokens (entrÃ©e/sortie): `openai_request_tokens` et `openai_response_tokens`
    -   InterprÃ©tation: `increase(...[5m])` pour suivre la consommation sur la fenÃªtre. Sert dâ€™estimateur de coÃ»t.
-   FAISS â€” volume de recherches: `faiss_search_total{index,endpoint}`
-   FAISS â€” latence p95: histogramme `faiss_search_duration_seconds_bucket{index,endpoint}`
-   FAISS â€” rÃ©sultats retournÃ©s: `faiss_results_count{index,endpoint}` (gauge ou histogramme simple selon implÃ©mentation)
-   SantÃ© application (pour contexte): `http_requests_total{method,endpoint,status}`, `http_request_duration_seconds_bucket{...}`, `http_requests_in_flight`.

Correspondance avec le dashboard OpenAI (captures dâ€™Ã©cran fournies):

-   Â« Total requests Â» â†” somme de `increase(openai_requests_total[1d])` (par modÃ¨le/endpoint si besoin).
-   Â« Total tokens Â» â†” `increase(openai_request_tokens[1d]) + increase(openai_response_tokens[1d])`.
-   Â« Spend Â» â†” estimation locale: \( cost \approx tokens*in/1000 * price*in + tokens_out/1000 * price_out \) par modÃ¨le. La valeur officielle de dÃ©pense reste celle du dashboard OpenAI; le graphe local sert dâ€™alerte prÃ©ventive.

### 2) Outils dâ€™intÃ©gration et restitution temps rÃ©el

-   Collecte: Prometheus scrute `web:8000/metrics` (voir `E3-E4/monitoring/prometheus/prometheus.yml`).
-   Restitution: Grafana (dashboards prÃ©â€‘provisionnÃ©s dans `E3-E4/monitoring/grafana/dashboards/`).
-   Alerting: Grafana Alerting avec envoi vers Discord (`E3-E4/monitoring/grafana/provisioning/alerting/`).

Exemples de panneaux Grafana (promql simplifiÃ©e):

```promql
// OpenAI â€” dÃ©bit (req/s)
sum by (model, endpoint, status) (rate(openai_requests_total[5m]))

// OpenAI â€” latence p95
histogram_quantile(0.95, sum by (le, model, endpoint) (rate(openai_request_duration_seconds_bucket[5m])))

// OpenAI â€” tokens sur 24h
sum(increase(openai_request_tokens[24h]) + increase(openai_response_tokens[24h]))

// FAISS â€” latence p95
histogram_quantile(0.95, sum by (le, index, endpoint) (rate(faiss_search_duration_seconds_bucket[5m])))
```

### 3) RÃ¨gles dâ€™alerte (extraits)

Les rÃ¨gles sont dÃ©claratives (voir `rules-fastapi.yaml`). Seuils proposÃ©s pour lâ€™environnement de dev, durcissables en prod:

-   Erreurs OpenAI anormales:
    -   Condition: `sum(rate(openai_requests_total{status="error"}[5m])) / sum(rate(openai_requests_total[5m])) > 0.1` pendant 2m.
-   Latence OpenAI Ã©levÃ©e (p95 > 2s):
    -   Condition: `histogram_quantile(0.95, rate(openai_request_duration_seconds_bucket[5m])) > 2` pendant 2m.
-   Latence FAISS Ã©levÃ©e (p95 > 500ms):
    -   Condition: `histogram_quantile(0.95, rate(faiss_search_duration_seconds_bucket[5m])) > 0.5` pendant 2m.
-   CoÃ»t en hausse (prÃ©â€‘alerte):
    -   Condition: `increase(openai_request_tokens[10m]) + increase(openai_response_tokens[10m]) > THRESHOLD_TOKENS` (seuil Ã  ajuster).

Les notifications sont acheminÃ©es vers Discord via `DISCORD_WEBHOOK_URL`.

### 4) AccessibilitÃ© de la restitution

-   Grafana est accessible via navigateur, supporte clavier et thÃ¨mes Ã  fort contraste; les dashboards utilisent palettes compatibles daltonisme.
-   Export CSV depuis chaque panel pour partage dans des feuilles de calcul (lecture non technique).
-   Snapshots Grafana pour parties prenantes sans compte et liens publics temporaires si besoin.

### 5) Bac Ã  sable / environnement de test

La chaÃ®ne est testable en local via Docker Compose:

```bash
cd E3-E4
docker compose up -d web prometheus grafana
```

VÃ©rifications:

-   `http://localhost:8001` (API), scrutÃ©e en interne sur `web:8000/metrics`.
-   `http://localhost:9090` (Prometheus) et `http://localhost:3000` (Grafana).
-   Lancer quelques appels Ã  lâ€™API pour faire Ã©voluer OpenAI/FAISS; les graphes rÃ©agissent en temps rÃ©el.

ScÃ©narios de test rapide:

-   surcharge latence: envoyer des requÃªtes successives avec contexte long; vÃ©rifier p95 OpenAI.
-   erreur contrÃ´lÃ©e: forcer un mauvais modÃ¨le/clÃ© pour gÃ©nÃ©rer des erreurs et tester lâ€™alerte Â« OpenAI Error Rate Â».
-   requÃªtes FAISS: dÃ©clencher des recherches pour observer le p95 FAISS.

### 6) ChaÃ®ne opÃ©rationnelle et correspondance aux critÃ¨res C11

-   Les mÃ©triques ciblÃ©es (OpenAI/FAISS) sont documentÃ©es et utilisÃ©es sans ambiguÃ¯tÃ©.
-   Les outils (Prometheus, Grafana, Discord) sont adaptÃ©s aux contraintes du projet et dÃ©jÃ  intÃ©grÃ©s.
-   Restitution temps rÃ©el: dashboards Grafana + export CSV; validation croisÃ©e avec le dashboard OpenAI.
-   AccessibilitÃ©: partage par snapshots/CSV; thÃ¨mes Ã  fort contraste; navigation clavier.
-   Bac Ã  sable: exÃ©cution via Docker Compose, rÃ¨gles dâ€™alerte testÃ©es.
-   ChaÃ®ne en Ã©tat de marche: mÃ©triques exposÃ©es par lâ€™API, collectÃ©es et affichÃ©es; alertes actives.
-   Sources versionnÃ©es: voir `E3-E4/monitoring/` dans le dÃ©pÃ´t.
-   Documentation dâ€™installation/configuration: voir section Â« DÃ©ploiement et exploitation locale Â» ciâ€‘dessous et `.env_example`.
-   ConformitÃ© accessibilitÃ©: documentation fournie en Markdown lisible, compatible lecteurs dâ€™Ã©cran, contrastes forts sur dashboards.

---

## Monitoring de lâ€™application dâ€™IA â€” Rapport de certification (C20)

Ce document prÃ©sente lâ€™architecture et la mise en Å“uvre du monitorage de lâ€™application dâ€™intelligence artificielle dÃ©veloppÃ©e dans le cadre du projet `E3-E4`. Lâ€™application est constituÃ©e dâ€™une API FastAPI intÃ©grant un modÃ¨le LLM (OpenAI), un moteur de recherche sÃ©mantique basÃ© sur FAISS et une base de donnÃ©es PostgreSQL. ConformÃ©ment Ã  la compÃ©tence C20, lâ€™objectif du dispositif de monitoring est de surveiller la disponibilitÃ© et les performances des composants, de dÃ©tecter automatiquement les incidents et dâ€™alimenter une boucle de rÃ©troaction MLOps permettant lâ€™amÃ©lioration continue du systÃ¨me, tout en respectant les principes de protection des donnÃ©es personnelles.

### Architecture de monitorage

Lâ€™architecture de surveillance repose sur la collecte de mÃ©triques instrumentÃ©es au sein de lâ€™API et lâ€™agrÃ©gation de mÃ©triques systÃ¨mes et bases de donnÃ©es, lesquelles sont centralisÃ©es puis visualisÃ©es. ConcrÃ¨tement, lâ€™application FastAPI expose un endpoint `/metrics` conforme au format Prometheus. Un service Prometheus, dÃ©ployÃ© en environnement local via Docker Compose, interroge pÃ©riodiquement cet endpoint ainsi que deux exporters standards: un exporter PostgreSQL pour les mÃ©triques de la base et un node exporter pour lâ€™hÃ´te. Les sÃ©ries temporelles collectÃ©es sont consultÃ©es depuis un service Grafana, qui propose des tableaux de bord prÃ©â€‘provisionnÃ©s et un moteur dâ€™alerting. Cette architecture, simple et modulaire, garantit une sÃ©paration claire des responsabilitÃ©s: lâ€™application se concentre sur lâ€™exposition de signaux pertinents, tandis que Prometheus et Grafana assurent respectivement la collecte, la conservation et lâ€™exploitation des donnÃ©es de supervision.

La configuration de Prometheus prÃ©cise des jobs de scraping pour lâ€™application, lâ€™exporter PostgreSQL et le node exporter, avec un intervalle adaptÃ© Ã  la criticitÃ© de chaque source. Lâ€™extrait suivant illustre le paramÃ©trage du job dÃ©diÃ© Ã  lâ€™API FastAPI, qui interroge lâ€™endpoint `/metrics` toutes les dix secondes.

```yaml
scrape_configs:
    - job_name: "fastapi-app"
      static_configs:
          - targets: ["web:8000"]
      metrics_path: "/metrics"
      scrape_interval: 10s
```

### Instrumentation applicative (FastAPI, OpenAI, FAISS)

Lâ€™instrumentation applicative est rÃ©alisÃ©e au plus prÃ¨s des parcours utilisateurs et des intÃ©grations IA. Un middleware HTTP dans `app.py` mesure le volume de requÃªtes, la latence par point dâ€™entrÃ©e et le nombre de requÃªtes en cours, et catÃ©gorise les erreurs en distinguant erreurs clientes et erreurs serveur. Cette approche permet dâ€™obtenir une vision fiable de lâ€™expÃ©rience perÃ§ue par les utilisateurs, notamment via le suivi de percentiles de latence. Auâ€‘delÃ  de la couche HTTP, lâ€™application instrumente les appels au modÃ¨le OpenAI ainsi que les recherches FAISS. Les mÃ©triques exposÃ©es incluent le nombre de requÃªtes LLM par modÃ¨le et par endpoint, la durÃ©e dâ€™exÃ©cution des appels, le volume de tokens envoyÃ©s et reÃ§us, la durÃ©e des recherches FAISS et le nombre de rÃ©sultats retournÃ©s. Lâ€™API expose Ã©galement des compteurs dÃ©crivant lâ€™activitÃ© conversationnelle (conversations par statut et messages par type) afin de relier lâ€™activitÃ© fonctionnelle aux indicateurs techniques. Lâ€™endpoint dâ€™exposition des mÃ©triques est minimaliste et sâ€™appuie directement sur la bibliothÃ¨que `prometheus_client`.

```python
# E3-E4/fastapi/app.py
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

### MÃ©triques Ã  risque et seuils dâ€™alerte

Le dispositif dâ€™alerte sâ€™appuie sur les rÃ¨gles de Grafana, provisionnÃ©es de maniÃ¨re dÃ©clarative. Trois scÃ©narios dâ€™incident majeurs sont couverts. Dâ€™abord, lâ€™Ã©lÃ©vation anormale du taux dâ€™erreurs HTTP est dÃ©tectÃ©e par une rÃ¨gle calculant le taux dâ€™occurrence des rÃ©ponses dont le code est de la famille 4xx ou 5xx sur une fenÃªtre de cinq minutes; un dÃ©passement du seuil fixÃ© Ã  dix pour cent maintenu pendant deux minutes dÃ©clenche une alerte de sÃ©vÃ©ritÃ© Â« warning Â». Ensuite, la dÃ©gradation des performances est surveillÃ©e au travers du quantile 95 de la distribution des latences HTTP; lorsque le p95 excÃ¨de deux secondes pendant deux minutes sur une fenÃªtre glissante de cinq minutes, une alerte Â« warning Â» est Ã©mise. Enfin, lâ€™indisponibilitÃ© du service est traitÃ©e par une rÃ¨gle sâ€™appuyant sur la mÃ©trique `up` issue de Prometheus: si le job `fastapi-app` est indisponible pendant une minute, une alerte de sÃ©vÃ©ritÃ© Â« critical Â» est gÃ©nÃ©rÃ©e. Ces seuils sont volontairement pragmatiques pour lâ€™environnement local et peuvent Ãªtre durcis en production en fonction des objectifs de service.

Ã€ titre dâ€™illustration, lâ€™expression PromQL utilisÃ©e pour dÃ©tecter un temps de rÃ©ponse excessif sâ€™appuie sur la fonction `histogram_quantile` appliquÃ©e au taux des buckets dâ€™histogramme de latence: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2`. De maniÃ¨re similaire, le taux dâ€™erreurs est Ã©valuÃ© via `rate(http_requests_total{status=~"4..|5.."}[5m]) > 0.1`, tandis que la dÃ©tection dâ€™indisponibilitÃ© repose sur `up{job="fastapi-app"} == 0`. Les notifications sont Ã©mises vers un canal Discord par lâ€™intermÃ©diaire dâ€™un Â« contact point Â» configurÃ© Ã  partir de la variable dâ€™environnement `DISCORD_WEBHOOK_URL`.

```yaml
# E3-E4/monitoring/grafana/provisioning/alerting/contact-points.yaml
contactPoints:
    - name: discord-default
      receivers:
          - uid: discord-webhook
            type: discord
            settings:
                url: "${DISCORD_WEBHOOK_URL}"
                message: |
                    ğŸš¨ Alerte Grafana: {{ .CommonLabels.alertname }}\nStatut: {{ .Status }}
```

### DÃ©ploiement et exploitation locale

Le dispositif est entiÃ¨rement opÃ©rable en environnement local au moyen de Docker Compose. Une fois les variables dâ€™environnement renseignÃ©es (voir `.env_example`, notamment `DISCORD_WEBHOOK_URL`, `OPENAI_API_KEY` et `OPENAI_MODEL`), le lancement de la commande `docker-compose up -d` provisionne lâ€™application, Prometheus, Grafana ainsi que les exporters PostgreSQL et Node. Prometheus conserve les sÃ©ries temporelles avec une rÃ©tention adaptÃ©e Ã  lâ€™environnement de dÃ©veloppement (environ deux cents heures) et expose sa console sur `http://localhost:9090`. Grafana est accessible sur `http://localhost:3000` avec des identifiants par dÃ©faut en local et charge automatiquement la source de donnÃ©es Prometheus ainsi que les tableaux de bord fournis, dont un tableau de bord dÃ©diÃ© Ã  FastAPI. Lâ€™application est accessible Ã  lâ€™adresse `http://localhost:8001`, tandis que le scraping des mÃ©triques sâ€™effectue au sein du rÃ©seau Docker sur `web:8000/metrics`.

### Journalisation et conformitÃ© des donnÃ©es personnelles

La journalisation applicative utilise le module standard `logging` et se limite Ã  des Ã©vÃ©nements techniques contextualisÃ©s (succÃ¨s ou Ã©chec dâ€™appel OpenAI, durÃ©es dâ€™exÃ©cution FAISS, statut des conversations), sans journaliser de contenu conversationnel ou de donnÃ©es identifiantes. Les mÃ©triques exposÃ©es par `prometheus_client` sont agrÃ©gÃ©es et utilisent des labels non identifiants (mÃ©thode, endpoint, statut HTTP, modÃ¨le), ce qui satisfait au principe de minimisation des donnÃ©es. Les secrets et informations sensibles sont fournis par variables dâ€™environnement, et lâ€™accÃ¨s Ã  lâ€™endpoint `/metrics` est conÃ§u pour rester interne au rÃ©seau dans une perspective de dÃ©ploiement en production. Ces choix assurent un niveau de conformitÃ© cohÃ©rent avec les exigences de protection des donnÃ©es dans un contexte dâ€™IA appliquÃ©e.

### Suivi et consultation des logs

En dÃ©veloppement, les logs de lâ€™application et des services sont consultÃ©s directement dans Docker Desktop (onglet Â« Logs Â» de chaque conteneur) ou en ligne de commande, selon prÃ©fÃ©rence.

```bash
docker compose logs -f web
docker compose logs -f prometheus grafana
```

En production, les logs sont consultÃ©s via Portainer dans la vue Â« Logs Â» de chaque conteneur. La consultation en CLI reste possible si nÃ©cessaire.

```bash
docker logs -f <container_name>
```

Ce choix privilÃ©gie la simplicitÃ©: je connais des solutions de centralisation comme Grafana Loki avec Promtail (ou des stacks ELK/EFK), trÃ¨s utiles pour la recherche pleinâ€‘texte, la corrÃ©lation et la rÃ©tention avancÃ©e, mais elles seraient surdimensionnÃ©es pour cette application. Elles ajoutent des composants Ã  exploiter (agents, stockage, politiques de rÃ©tention) sans bÃ©nÃ©fice immÃ©diat. Les mÃ©triques et alertes couvrent les signaux forts; lâ€™inspection des logs conteneurisÃ©s via Docker Desktop et Portainer suffit aujourdâ€™hui. Si la volumÃ©trie, les exigences dâ€™audit ou les SLO Ã©voluent, une centralisation pourra Ãªtre introduite ultÃ©rieurement sans modification du code applicatif.

### ProcÃ©dures de test des alertes

La validitÃ© des rÃ¨gles dâ€™alerte est vÃ©rifiÃ©e par des scÃ©narios dâ€™inductions contrÃ´lÃ©es. Lâ€™indisponibilitÃ© du service est testÃ©e en arrÃªtant temporairement le conteneur `web`, ce qui entraÃ®ne, aprÃ¨s une minute, lâ€™Ã©mission dâ€™une alerte critique Â« ServiceDown Â». Le taux dâ€™erreurs est Ã©prouvÃ© en forÃ§ant des rÃ©ponses 5xx sur un endpoint de test pendant plus de deux minutes, permettant dâ€™observer le dÃ©clenchement de lâ€™alerte Â« HighErrorRate Â». La surveillance de la latence est Ã©valuÃ©e en introduisant, cÃ´tÃ© application, une temporisation volontaire portant la durÃ©e de traitement auâ€‘delÃ  de deux secondes, puis en soumettant la route concernÃ©e Ã  une charge continue pendant environ cinq minutes; lâ€™alerte Â« HighResponseTime Â» doit alors apparaÃ®tre. Dans chaque cas, la rÃ©ception des notifications sur Discord et lâ€™Ã©volution de lâ€™Ã©tat dans Grafana (ouverture, acquittement, rÃ©solution) sont contrÃ´lÃ©es.

### Boucle de rÃ©troaction MLOps

Les mÃ©triques et alertes alimentent une boucle de rÃ©troaction qui structure lâ€™amÃ©lioration continue du systÃ¨me. Les indicateurs de latence et dâ€™erreur des appels OpenAI conduisent Ã  revisiter les choix de modÃ¨les, les paramÃ¨tres de timeout, la stratÃ©gie de retry et la conception des prompts. Les mesures de performance de FAISS, notamment les temps de recherche et le nombre de rÃ©sultats pertinents retournÃ©s, Ã©clairent lâ€™optimisation de lâ€™index, la valeur de `k` et la qualitÃ© du corpus indexÃ©. Le suivi des volumes de tokens permet de maÃ®triser les coÃ»ts en adaptant la longueur du contexte et en rationalisant les prompts. Enfin, lâ€™analyse du taux dâ€™erreurs HTTP et des exceptions corrÃ©lÃ©es aboutit Ã  des correctifs ciblÃ©s sur les endpoints et Ã  lâ€™introduction de mÃ©canismes de repli et de backoff. Cette dÃ©marche est consignÃ©e et opÃ©rÃ©e de maniÃ¨re itÃ©rative pour renforcer conjointement robustesse, performance et efficience Ã©conomique.

### Alignement avec la compÃ©tence C20

Le dispositif prÃ©sentÃ© rÃ©pond aux attendus de la compÃ©tence C20. Les mÃ©triques clÃ©s liÃ©es aux risques (erreurs, latence, disponibilitÃ©, coÃ»ts via tokens) sont dÃ©finies et assorties de seuils et dâ€™alertes effectifs. Les choix technologiques â€” instrumentation `prometheus_client` dans lâ€™API, collecte par Prometheus, visualisation et alerting par Grafana, exporters Postgres et Node â€” sont justifiÃ©s par leur maturitÃ©, leur faible empreinte et leur interopÃ©rabilitÃ©. Lâ€™ensemble des outils est installÃ© et opÃ©rationnel en environnement local, et lâ€™instrumentation nÃ©cessaire est intÃ©grÃ©e au code source dans `app.py` et `monitoring_utils.py`. Les alertes, enfin, sont configurÃ©es et fonctionnelles avec un acheminement vers Discord; elles constituent le point de dÃ©part dâ€™actions correctrices concrÃ¨tes dans la boucle MLOps. Le respect de la protection des donnÃ©es personnelles est pris en compte par la minimisation des informations exposÃ©es, lâ€™absence de journalisation de contenu sensible et lâ€™usage de variables dâ€™environnement pour les secrets.

### Annexes techniques

Lâ€™exemple suivant rappelle le principe dâ€™exposition des mÃ©triques cÃ´tÃ© application, qui repose sur la gÃ©nÃ©ration du format Prometheus Ã  partir du registre interne de `prometheus_client`.

```python
# E3-E4/fastapi/app.py
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

Le contact de notification Grafana est dÃ©fini pour diffuser les alertes vers Discord Ã  lâ€™aide dâ€™un webhook configurable par variable dâ€™environnement.

```yaml
# E3-E4/monitoring/grafana/provisioning/alerting/contact-points.yaml
contactPoints:
    - name: discord-default
      receivers:
          - uid: discord-webhook
            type: discord
            settings:
                url: "${DISCORD_WEBHOOK_URL}"
                message: |
                    ğŸš¨ Alerte Grafana: {{ .CommonLabels.alertname }}\nStatut: {{ .Status }}
```

Pour la collecte, Prometheus interroge lâ€™API Ã  un intervalle de dix secondes, ce qui fournit une rÃ©solution suffisante pour lâ€™observation fine des pics de latence et des incidents en environnement de dÃ©veloppement.

```yaml
# E3-E4/monitoring/prometheus/prometheus.yml
scrape_configs:
    - job_name: "fastapi-app"
      static_configs:
          - targets: ["web:8000"]
      metrics_path: "/metrics"
      scrape_interval: 10s
```

### Fonctionnement de Prometheus

Prometheus adopte un modÃ¨le de collecte de type Â« pull Â» dans lequel le serveur Prometheus interroge pÃ©riodiquement des cibles HTTP exposant des mÃ©triques au format texte. Chaque cible â€” lâ€™application FastAPI, lâ€™exporter PostgreSQL et le node exporter â€” publie un endpoint HTTP (`/metrics`) dÃ©crivant lâ€™Ã©tat de la cible sous forme de sÃ©ries temporelles nommÃ©es, enrichies de labels. Ã€ chaque scrutation, Prometheus Ã©chantillonne la valeur courante de ces sÃ©ries et les stocke dans sa base de donnÃ©es temporelle (TSDB) locale, en conservant pour chaque point un horodatage, une valeur et le jeu de labels associÃ©. La TSDB organise les donnÃ©es par Â« chunks Â» sur disque avec un mÃ©canisme de compaction, ce qui permet des requÃªtes temporelles efficaces et une rÃ©tention configurable adaptÃ©e aux contraintes de lâ€™environnement.

Les types de mÃ©triques standard â€” Counter, Gauge, Histogram et Summary â€” rÃ©pondent Ã  des usages distincts. Les compteurs sont strictement croissants et mesurent des Ã©vÃ©nements (par exemple `http_requests_total`). Les jauges reprÃ©sentent des grandeurs instantanÃ©es pouvant monter ou descendre (par exemple `http_requests_in_flight`). Les histogrammes accumulent des observations dans des Â« buckets Â» de seuils croissants et exposent automatiquement des sÃ©ries `*_bucket`, `*_count` et `*_sum` permettant dâ€™estimer des quantiles avec `histogram_quantile`. Les summaries calculent localement des quantiles, mais dans ce projet, les histogrammes ont Ã©tÃ© privilÃ©giÃ©s pour permettre lâ€™agrÃ©gation cÃ´tÃ© serveur. Dans notre configuration, lâ€™intervalle de scrutation est de dix secondes pour lâ€™application, ce qui offre une rÃ©solution suffisante pour suivre les pics de latence et de charge en phase de dÃ©veloppement, tout en limitant la cardinalitÃ© et le volume de donnÃ©es.

Lors de lâ€™exÃ©cution, Prometheus Ã©tiquette chaque sÃ©rie avec des labels clÃ©/valeur (par exemple `method`, `endpoint`, `status` pour les mÃ©triques HTTP, `model` et `endpoint` pour les mÃ©triques OpenAI). Ces labels rendent possible des agrÃ©gations par dimension (somme ou moyenne par endpoint, quantiles par modÃ¨le, etc.). Le serveur expose ensuite une API de requÃªtage (PromQL) utilisÃ©e Ã  la fois par lâ€™interface Prometheus et par Grafana pour rÃ©aliser des graphiques, des statistiques et des rÃ¨gles dâ€™alerte. Enfin, la mÃ©trique spÃ©ciale `up` permet de vÃ©rifier la disponibilitÃ© dâ€™une cible: une valeur de `1` indique que le scrape a rÃ©ussi, `0` signale une indisponibilitÃ©.

### Fonctionnement de Grafana

Grafana se connecte Ã  Prometheus en tant que source de donnÃ©es et fournit une couche de visualisation, de tableaux de bord et dâ€™alerting. Dans ce projet, la source de donnÃ©es Prometheus est provisionnÃ©e automatiquement au dÃ©marrage de Grafana. Les tableaux de bord sont dÃ©crits au format JSON et montÃ©s dans le conteneur, puis Grafana les charge selon la configuration de provisioning. Chaque panneau dâ€™un tableau de bord contient une ou plusieurs requÃªtes PromQL et un rendu (courbe temporelle, statistique unique, jauge, etc.). Grafana permet dâ€™appliquer des transformations simples (agrÃ©gations, changements dâ€™unitÃ©, renommage), de configurer des seuils visuels et de dÃ©finir des intervalles de rafraÃ®chissement.

Le moteur dâ€™alerting de Grafana Ã©value pÃ©riodiquement des expressions PromQL associÃ©es Ã  des conditions sur une durÃ©e. Lorsquâ€™une condition est satisfaite pendant la fenÃªtre dâ€™Ã©valuation, une alerte passe Ã  lâ€™Ã©tat Â« firing Â». Les notifications sont expÃ©diÃ©es via des Â« contact points Â»; ici, un webhook Discord paramÃ©trÃ© par la variable `DISCORD_WEBHOOK_URL` diffuse un message rÃ©sumant lâ€™alerte, sa gravitÃ© et un lien de contexte vers Grafana. Cette approche centralise la gestion des alertes au mÃªme endroit que les dashboards tout en se reposant sur Prometheus pour lâ€™exÃ©cution des requÃªtes sousâ€‘jacentes.

### IntÃ©gration et exploitation des mÃ©triques OpenAI

Les mÃ©triques OpenAI sont instrumentÃ©es directement dans lâ€™API FastAPI. Des compteurs et histogrammes sont dÃ©clarÃ©s dans `app.py` avec les labels `model` et `endpoint`. Des dÃ©corateurs et context managers dÃ©finis dans `monitoring_utils.py` enveloppent les appels au LLM et enregistrent pour chaque exÃ©cution le succÃ¨s ou lâ€™Ã©chec (`openai_requests_total` avec `status`), la durÃ©e dâ€™exÃ©cution (`openai_request_duration_seconds` via la sÃ©rie `*_bucket`) ainsi que le volume de tokens utilisÃ©s (`openai_request_tokens` et `openai_response_tokens` lorsque lâ€™information est disponible). Lâ€™application expose ces sÃ©ries sur `/metrics` et Prometheus les scrute Ã  intervalle rÃ©gulier. Dans Grafana, les panneaux dÃ©diÃ©s se contentent dâ€™interroger Prometheus en PromQL.

Le taux de requÃªtes OpenAI sâ€™obtient par la dÃ©rivÃ©e temporelle du compteur `openai_requests_total` avec la fonction `rate` sur une fenÃªtre de cinq minutes. La latence perÃ§ue est estimÃ©e par `histogram_quantile(0.95, rate(openai_request_duration_seconds_bucket[5m]))`, qui reconstruit le quantile 95 Ã  partir des buckets dâ€™histogramme. Il est ensuite possible de filtrer par modÃ¨le ou endpoint Ã  lâ€™aide des labels, ou de regrouper les sÃ©ries pour calculer des agrÃ©gations par dimension. Si lâ€™on souhaite suivre les coÃ»ts, il suffit dâ€™ajouter un panneau interrogeant `increase(openai_request_tokens[5m])` et `increase(openai_response_tokens[5m])` pour mesurer la consommation de tokens sur la fenÃªtre.

### Explication des mÃ©triques des dashboards Grafana

Le tableau de bord Â« FastAPI Monitoring Dashboard Â» regroupe les principaux indicateurs de lâ€™API et des intÃ©grations IA. Le panneau Â« Request Rate Â» reprÃ©sente le dÃ©bit de requÃªtes HTTP, calculÃ© par `rate(http_requests_total[5m])` et ventilÃ© par mÃ©thode et endpoint. Il permet dâ€™identifier les pÃ©riodes de charge et de vÃ©rifier la stabilitÃ© du trafic. Le panneau Â« Response Time Â» affiche le 95e percentile des temps de rÃ©ponse Ã  lâ€™aide de `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))`. Une Ã©lÃ©vation persistante de ce quantile traduit une dÃ©gradation de lâ€™expÃ©rience utilisateur, et constitue souvent un prÃ©curseur dâ€™incident plus large. Le panneau Â« Error Rate Â» suit le taux dâ€™erreurs HTTP (codes 4xx et 5xx) via `rate(http_requests_total{status=~"4..|5.."}[5m])`. Une montÃ©e des 5xx signale un problÃ¨me serveur; les 4xx peuvent rÃ©vÃ©ler un dÃ©faut de validation ou dâ€™usage cÃ´tÃ© client.

Un panneau de type statistique, Â« Active Connections Â», agrÃ¨ge `http_requests_in_flight` pour fournir une mesure instantanÃ©e de la charge en cours de traitement. Deux panneaux dâ€™infrastructure, Â« CPU Usage Â» et Â« Memory Usage Â», proviennent du node exporter. Le premier estime lâ€™utilisation CPU par `100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`, ce qui reflÃ¨te la part non idle du CPU. Le second calcule le pourcentage de mÃ©moire utilisÃ©e Ã  partir des mÃ©triques `node_memory_MemTotal_bytes` et `node_memory_MemAvailable_bytes`. Ces deux panneaux permettent de corrÃ©ler les anomalies applicatives avec la saturation Ã©ventuelle de la machine hÃ´te.

Les intÃ©grations IA disposent de trois panneaux dÃ©diÃ©s. Â« OpenAI Requests Rate Â» retrace le dÃ©bit dâ€™appels LLM via `rate(openai_requests_total[5m])`, libellÃ© par modÃ¨le, endpoint et statut; il met en Ã©vidence la charge adressÃ©e au fournisseur et la proportion dâ€™erreurs. Â« OpenAI Response Time Â» prÃ©sente le p95 des durÃ©es des appels au modÃ¨le grÃ¢ce Ã  `histogram_quantile(0.95, rate(openai_request_duration_seconds_bucket[5m]))`, indicateur clef pour lâ€™UX et les timeouts. Â« FAISS Search Duration Â» restitue le p95 des durÃ©es de recherche via `histogram_quantile(0.95, rate(faiss_search_duration_seconds_bucket[5m]))`. Une hausse simultanÃ©e de ces latences oriente lâ€™investigation vers la saturation I/O, la taille de contexte, ou la configuration de lâ€™index FAISS. Deux panneaux de synthÃ¨se, Â« Chatbot Conversations Â» et Â« Chatbot Messages Â», exposent respectivement `chatbot_conversations_total` (par statut) et `chatbot_messages_total` (par type). Ils relient lâ€™activitÃ© fonctionnelle Ã  la santÃ© technique du systÃ¨me.

Le tableau de bord Â« PostgreSQL Monitoring Dashboard Â» se focalise sur lâ€™activitÃ© de la base. Le panneau Â« Active Connections Â» affiche `pg_stat_database_numbackends`, indicateur de connexions actives par base. Une croissance rapide peut signaler un pool mal dimensionnÃ© ou des fuites de connexions. Â« Database Size Â» sâ€™appuie sur `pg_database_size_bytes` pour suivre lâ€™Ã©volution du volume occupÃ© par les donnÃ©es, utile pour planifier la capacitÃ© et dÃ©tecter une croissance inattendue. Â« Transactions per Second Â» combine `rate(pg_stat_database_xact_commit[5m])` et `rate(pg_stat_database_xact_rollback[5m])` pour fournir un dÃ©bit global de transactions, reflet direct de lâ€™activitÃ© applicative. Enfin, Â« Cache Hit Ratio Â» mesure lâ€™efficacitÃ© du cache en calculant `rate(pg_stat_database_blks_hit[5m]) / (rate(pg_stat_database_blks_hit[5m]) + rate(pg_stat_database_blks_read[5m]))`. Un ratio Ã©levÃ©, proche de 1, indique que la plupart des lectures sont servies depuis le cache; une chute durable invite Ã  revoir la taille des buffers ou les schÃ©mas dâ€™accÃ¨s.

Dans lâ€™ensemble, ces dashboards offrent une vue cohÃ©rente de boutâ€‘enâ€‘bout: charge et qualitÃ© de service cÃ´tÃ© HTTP, performance et coÃ»t cÃ´tÃ© LLM, pertinence et latence cÃ´tÃ© FAISS, ainsi que lâ€™Ã©tat de lâ€™infrastructure et de la base de donnÃ©es. Leur lecture conjointe permet de diagnostiquer efficacement les incidents, dâ€™identifier les goulets dâ€™Ã©tranglement et dâ€™alimenter la boucle dâ€™amÃ©lioration continue MLOps en donnÃ©es objectives.

## C21 â€” RÃ©solution dâ€™un incident technique (Migrations Alembic)

Cette section documente en dÃ©tail un incident de production liÃ© aux migrations Alembic et dÃ©crit la solution mise en place pour en garantir la nonâ€‘rÃ©gression. Alembic est lâ€™outil de migration de schÃ©ma de SQLAlchemy. Il versionne lâ€™Ã©volution de la base au moyen de fichiers de rÃ©vision qui dÃ©crivent des opÃ©rations DDL (crÃ©ation/modification de tables et de colonnes, contraintes, index, clÃ©s Ã©trangÃ¨resâ€¦). Le cycle attendu est le suivant: aprÃ¨s une modification des modÃ¨les SQLAlchemy, on gÃ©nÃ¨re une rÃ©vision (souvent via lâ€™autogÃ©nÃ©ration), on relit le script produit pour valider types et contraintes, puis on applique la migration Ã  la base avec `alembic upgrade head`. Sans cette derniÃ¨re Ã©tape, le code et la base divergent.

Lâ€™incident sâ€™est produit prÃ©cisÃ©ment lorsque quâ€™une rÃ©vision Ã©tait prÃ©sente dans le dÃ©pÃ´t mais que la base nâ€™avait pas reÃ§u `alembic upgrade head` au moment du dÃ©ploiement. Au dÃ©marrage, lâ€™application sâ€™attend Ã  un schÃ©ma Â« Ã  jour Â» (par exemple une nouvelle colonne dans `users`), tandis que la base expose encore lâ€™ancien schÃ©ma. Cette discordance provoque des erreurs SQL au dÃ©marrage ou lors des premiÃ¨res requÃªtes, typiquement des exceptions `UndefinedColumn` ou `relation does not exist`. Dans notre cas, lâ€™erreur survenait au moment de lâ€™initialisation (crÃ©ation de lâ€™administrateur par dÃ©faut), lorsque lâ€™ORM interrogeait la table `users` avec une structure non encore dÃ©ployÃ©e.

Pour reproduire lâ€™incident en dÃ©veloppement, jâ€™ai dâ€™abord dÃ©marrÃ© la stack afin dâ€™initialiser la base, puis jâ€™ai modifiÃ© un modÃ¨le dans `E3-E4/fastapi/models.py` afin dâ€™introduire un changement de schÃ©ma. AprÃ¨s gÃ©nÃ©ration dâ€™une rÃ©vision avec `alembic revision --autogenerate`, jâ€™ai relancÃ© le service applicatif sans appliquer `alembic upgrade head`. Lâ€™application a alors Ã©chouÃ© comme attendu, confirmant que la cause racine Ã©tait un drift entre le schÃ©ma attendu par le code et lâ€™Ã©tat rÃ©el de la base.

La procÃ©dure de dÃ©bogage repose sur trois vÃ©rifications. PremiÃ¨rement, lâ€™inspection des logs applicatifs permet dâ€™identifier lâ€™exception SQL et le moment prÃ©cis de lâ€™Ã©chec (dÃ©marrage ou premiÃ¨re requÃªte). DeuxiÃ¨mement, le diagnostic de lâ€™Ã©tat des migrations (`alembic current` et `alembic history`) vÃ©rifie que la base est bien positionnÃ©e sur la derniÃ¨re rÃ©vision (le Â« head Â»). TroisiÃ¨mement, une vÃ©rification de la configuration (`DATABASE_URL`) sâ€™assure que les commandes Alembic ciblent la bonne instance de base. Cette mÃ©thode isole rapidement la cause en Ã©vitant les confusions liÃ©es Ã  lâ€™environnement.

La solution mise en Å“uvre consiste Ã  rendre lâ€™application autoâ€‘rattrapante au dÃ©marrage. ConcrÃ¨tement, la commande de lancement du conteneur `web` exÃ©cute dÃ©sormais `alembic upgrade head` juste avant Uvicorn. Lorsquâ€™une migration est en attente, elle sâ€™applique immÃ©diatement; lorsquâ€™il nâ€™y en a pas, la commande est sans effet. Ce choix supprime lâ€™erreur humaine rÃ©currente dâ€™oublier `upgrade` lors des dÃ©ploiements, garantit lâ€™alignement durable entre code et schÃ©ma, et reste idempotent. Sur PostgreSQL, la plupart des DDL sont transactionnels, ce qui limite lâ€™exposition Ã  des Ã©tats intermÃ©diaires. La contrepartie, minime, est un lÃ©ger allongement du temps de dÃ©marrage lorsquâ€™une migration est effectivement appliquÃ©e.

La commande de dÃ©marrage est la suivante:

```dockerfile
CMD ["/bin/sh", "-c", "alembic upgrade head && uvicorn app:app --host 0.0.0.0 --port 8000"]
```

Deux bonnes pratiques complÃ¨tent ce dispositif. Dâ€™une part, mÃªme en autogÃ©nÃ©ration, chaque rÃ©vision doit Ãªtre relue avant application pour valider les types, defaults, contraintes et clÃ©s Ã©trangÃ¨res. Dâ€™autre part, lorsque lâ€™on introduit Alembic sur une base dÃ©jÃ  peuplÃ©e et historiquement crÃ©Ã©e sans migrations, il convient dâ€™initialiser lâ€™historique avec un Â« stamp Â» sur la rÃ©vision initiale, de faÃ§on Ã  aligner lâ€™Ã©tat logique des migrations avec lâ€™Ã©tat rÃ©el du schÃ©ma sans tenter de recrÃ©er des objets existants.

Enfin, la rÃ©solution a Ã©tÃ© documentÃ©e et versionnÃ©e. Elle comprend lâ€™ajout de la configuration Alembic (`alembic.ini` et rÃ©pertoire `alembic/` avec ses rÃ©visions), la crÃ©ation dâ€™une premiÃ¨re rÃ©vision initiale alignÃ©e avec les modÃ¨les et la mise Ã  jour du `Dockerfile` pour inclure lâ€™exÃ©cution de `alembic upgrade head` avant `uvicorn`. Une merge request dÃ©diÃ©e (Â« C21: IntÃ©gration Alembic + exÃ©cution auto des migrations Â») consigne le diagnostic, la reproduction, la procÃ©dure de dÃ©bogage et la solution retenue, afin dâ€™assurer la traÃ§abilitÃ© et la nonâ€‘rÃ©gression lors des prochains dÃ©ploiements.
